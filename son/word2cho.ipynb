{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import kss\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "choseong_list = [char for char in \"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text/CM.txt', encoding=\"utf-8\")\n",
    "i=0\n",
    "gogo= 0\n",
    "sentence_list = []\n",
    "lowCount = 7\n",
    "longCount = 20\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    if \"<!DOCTYPE\" in line:\n",
    "        gogo = 0\n",
    "    if \"<text>\" in line:\n",
    "        gogo = 1\n",
    "    if \"</text>\" in line:\n",
    "        gogo = 0\n",
    "    if gogo == 0: continue\n",
    "    if line != '\\n':\n",
    "        i=i+1\n",
    "        line = line.replace(u'\\u3000', u\"\")\n",
    "        line = line.replace(u'\\x80', u\"\")\n",
    "        line = line.replace(u'\\x84', u\"\")\n",
    "        line = line.replace(u'\\x88', u\"\")\n",
    "        line = line.replace(u'\\x8a', u\"\")\n",
    "        line = line.replace(u'\\x8b', u\"\")\n",
    "        line = line.replace(u'\\x8c', u\"\")\n",
    "        line = line.replace(u'\\x90', u\"\")\n",
    "        line = line.replace(u'\\x94', u\"\")\n",
    "        line = line.replace(u'\\x97', u\"\")\n",
    "        line = line.replace(u'\\x9a', u\"\")\n",
    "        line = line.replace(u'\\xa0', u\"\")\n",
    "        line = line.replace(u'\\x9e', u\"\")\n",
    "        line = line.replace(u'\\x9d', u\"\")\n",
    "        line = re.sub('<trunc>.+?<\\/trunc>', '', line, 0, re.S)\n",
    "        line = re.sub('<.+?>', '', line, 0, re.S)\n",
    "        line = re.sub('\\(.+?\\)', '', line, 0, re.S)\n",
    "        line = re.sub('\\[.+?\\]', '', line, 0, re.S)\n",
    "        line = re.sub('\\{.+?\\}', '', line, 0, re.S)\n",
    "        line = re.sub('《.+?》', '', line, 0, re.S)\n",
    "        line = re.sub('[-§¨©°³¸¼´·²～¯¥=+,#/ꮀ\\?:;¡^$@*\\\"※~&%ㆍ!『』\\‘|\\(\\)\\[\\]\\<\\>`\\'…《》\\{\\}_「」±√]', '', line,0,re.S)\n",
    "        line = line.lower()\n",
    "        line = re.sub('[a-z]', '', line, 0, re.S)\n",
    "        line = re.sub('[A-Z]', '', line, 0, re.S)\n",
    "        line_split = kss.split_sentences(line)\n",
    "        temp_line = []\n",
    "        for l_sp in line_split:\n",
    "            if len(l_sp) >lowCount and len(l_sp) <longCount:\n",
    "                temp_line.append(l_sp)\n",
    "        sentence_list+=temp_line\n",
    "f.close()\n",
    "print(len(sentence_list))\n",
    "print(sentence_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentence_list[:5])\n",
    "word_list = []\n",
    "target = []\n",
    "for sentence in sentence_list:\n",
    "    sen2word = \"\"\n",
    "    for word in sentence:\n",
    "        if re.match('[가-힣]',word) is not None :\n",
    "            sen2word+=choseong_list[(int)((ord(word)-ord('가'))/(21*28))]\n",
    "        else :\n",
    "            sen2word+=word\n",
    "    target.append(sentence+'\\n')\n",
    "    word_list.append(sen2word+'\\n')\n",
    "    \n",
    "#print(target[:5])\n",
    "#print(word_list[:5])\n",
    "src_vocab=set()\n",
    "for words in word_list: # 1줄씩 읽음\n",
    "    for char in words: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in target:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n",
    "        \n",
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "#print(src_vocab_size)\n",
    "#print(tar_vocab_size)\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab)\n",
    "print(tar_vocab[10:20])\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "#print(src_to_index)\n",
    "#print(tar_to_index)\n",
    "encoder_input = []\n",
    "for line in word_list: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X[:-1])\n",
    "print(encoder_input[:5])\n",
    "decoder_input = []\n",
    "for line in word_list:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(src_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])\n",
    "decoder_target = []\n",
    "for line in target:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_len = max([len(line) for line in word_list])-1\n",
    "max_tar_len = max([len(line) for line in target])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
    "\n",
    "encoder_inputer = encoder_input\n",
    "decoder_inputer = decoder_input\n",
    "decoder_targeter = decoder_target\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태.\n",
    "\n",
    "decoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timer in range(5):\n",
    "    encoder_inputer = encoder_inputer[timer*10000:(timer+1)*10000]\n",
    "    decoder_inputer = decoder_inputer[timer*10000:(timer+1)*10000]\n",
    "    decoder_targeter = decoder_targeter[timer*10000:(timer+1)*10000]\n",
    "\n",
    "    encoder_input = to_categorical(encoder_inputer, num_classes=src_vocab_size)\n",
    "    decoder_input = to_categorical(decoder_inputer, num_classes=src_vocab_size)\n",
    "    decoder_target = to_categorical(decoder_targeter, num_classes=tar_vocab_size)\n",
    "    \n",
    "    model.fit(x=[encoder_input, decoder_input], y=decoder_target, epochs=200)\n",
    "    #model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄱㄴㄷㄹㅇ  ㅇ13124124\n"
     ]
    }
   ],
   "source": [
    "def str2cho(input_str):\n",
    "    #문장을 입력받으면 초성 문자열로반환하는 코드\n",
    "    cho = \"\"\n",
    "    for word in input_str:\n",
    "        if re.match('[가-힣]',word) is not None :\n",
    "            cho+=(choseong_list[(int)((ord(word)-ord('가'))/(21*28))])\n",
    "        else :\n",
    "            cho+=(word)\n",
    "    return cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # 첫글자에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, src_vocab_size))\n",
    "    target_seq[0, 0, src_to_index[str2cho(input_seq[0])]] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 입력 글자를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > len(input_seq)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, src_vocab_size))\n",
    "        target_seq[0, 0, src_to_index[str2cho(sampled_char)]] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for seq_index in [105,7000,15502,40015,5]: # 입력 문장의 인덱스\n",
    "    i+=1\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', word_list[seq_index])\n",
    "    print('정답 문장:', target[seq_index][1:len(target[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "t= np.zeros((1, 1, 100))\n",
    "t[0,0,55] = 1\n",
    "targmax = np.argmax(t[0, -1, :])\n",
    "print(targmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
