{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import kss\n",
    "f = open('8CM00054.txt', encoding=\"utf8\")\n",
    "i=0\n",
    "sentence_list = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    if line != '\\n':\n",
    "        i=i+1\n",
    "        line = re.sub('<trunc>.+?<\\/trunc>', '', line, 0, re.S)\n",
    "        line = re.sub('<.+?>', '', line, 0, re.S)\n",
    "        line = re.sub('\\(.+?\\)', '', line, 0, re.S)\n",
    "        line = re.sub('\\[.+?\\]', '', line, 0, re.S)\n",
    "        line = re.sub('\\{.+?\\}', '', line, 0, re.S)\n",
    "        line = re.sub('《.+?》', '', line, 0, re.S)\n",
    "        line = re.sub('[-=+,#/\\?:^$@*\\\"※~&%ㆍ!『』\\‘|\\(\\)\\[\\]\\<\\>`\\'…《》\\{\\}_「」±√]', '', line,0,re.S)\n",
    "        line = line.lower()\n",
    "        if len(line) >4:\n",
    "            sentence_list+= kss.split_sentences(line)\n",
    "f.close()\n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이번 여름에', '그 아프가니스탄으로 여행을 갔다', '왔거든요', '아 근까 아프가니스탄에 직접 갈 수 있는', '그러니까 직항이 한국에서부터 없기 때문에']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t이번 여름에\\n', '\\t그 아프가니스탄으로 여행을 갔다\\n', '\\t왔거든요\\n', '\\t아 근까 아프가니스탄에 직접 갈 수 있는\\n', '\\t그러니까 직항이 한국에서부터 없기 때문에\\n']\n"
     ]
    }
   ],
   "source": [
    "choseong_list = [char for char in \"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"]\n",
    "word_list = []\n",
    "target = []\n",
    "for sentence in sentence_list:\n",
    "    sen2word = \"\"\n",
    "    for word in sentence:\n",
    "        if re.match('[가-힣]',word) is not None :\n",
    "            sen2word+=choseong_list[(int)((ord(word)-ord('가'))/(21*28))]\n",
    "        else :\n",
    "            sen2word+=word\n",
    "    target.append('\\t'+sentence+'\\n')\n",
    "    word_list.append(sen2word)\n",
    "    \n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅇㅂ ㅇㄹㅇ', 'ㄱ ㅇㅍㄱㄴㅅㅌㅇㄹ ㅇㅎㅇ ㄱㄷ', 'ㅇㄱㄷㅇ', 'ㅇ ㄱㄲ ㅇㅍㄱㄴㅅㅌㅇ ㅈㅈ ㄱ ㅅ ㅇㄴ', 'ㄱㄹㄴㄲ ㅈㅎㅇ ㅎㄱㅇㅅㅂㅌ ㅇㄱ ㄸㅁㅇ']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab=set()\n",
    "for words in word_list: # 1줄씩 읽음\n",
    "    for char in words: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in target:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "808\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', '1', '7', 'a', '·', 'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
      "['간', '갈', '감', '갑', '갔', '강', '갖', '같', '개', '객']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab)\n",
    "print(tar_vocab[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '.': 2, '1': 3, '7': 4, 'a': 5, '·': 6, 'ㄱ': 7, 'ㄲ': 8, 'ㄴ': 9, 'ㄷ': 10, 'ㄸ': 11, 'ㄹ': 12, 'ㅁ': 13, 'ㅂ': 14, 'ㅃ': 15, 'ㅅ': 16, 'ㅆ': 17, 'ㅇ': 18, 'ㅈ': 19, 'ㅉ': 20, 'ㅊ': 21, 'ㅋ': 22, 'ㅌ': 23, 'ㅍ': 24, 'ㅎ': 25}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '.': 4, '1': 5, '7': 6, 'a': 7, '·': 8, '가': 9, '각': 10, '간': 11, '갈': 12, '감': 13, '갑': 14, '갔': 15, '강': 16, '갖': 17, '같': 18, '개': 19, '객': 20, '걔': 21, '거': 22, '건': 23, '걷': 24, '걸': 25, '검': 26, '겁': 27, '것': 28, '게': 29, '겠': 30, '겨': 31, '격': 32, '겪': 33, '견': 34, '결': 35, '겸': 36, '겹': 37, '겼': 38, '경': 39, '계': 40, '고': 41, '곡': 42, '곤': 43, '곧': 44, '골': 45, '곰': 46, '곱': 47, '곳': 48, '공': 49, '과': 50, '관': 51, '괄': 52, '광': 53, '괜': 54, '굉': 55, '교': 56, '구': 57, '국': 58, '군': 59, '굳': 60, '굴': 61, '굶': 62, '굽': 63, '궁': 64, '권': 65, '귀': 66, '균': 67, '그': 68, '근': 69, '글': 70, '금': 71, '급': 72, '긍': 73, '기': 74, '긴': 75, '길': 76, '김': 77, '깄': 78, '깊': 79, '까': 80, '깐': 81, '깜': 82, '깝': 83, '깡': 84, '깨': 85, '꺼': 86, '껀': 87, '껄': 88, '께': 89, '꼈': 90, '꼬': 91, '꼭': 92, '꽃': 93, '꾸': 94, '꾼': 95, '꿀': 96, '꿈': 97, '꿔': 98, '꿨': 99, '뀌': 100, '끄': 101, '끊': 102, '끌': 103, '끔': 104, '끗': 105, '끝': 106, '끼': 107, '낀': 108, '낌': 109, '나': 110, '난': 111, '날': 112, '남': 113, '납': 114, '낫': 115, '났': 116, '낳': 117, '내': 118, '낸': 119, '낼': 120, '냅': 121, '냈': 122, '냐': 123, '냔': 124, '냥': 125, '너': 126, '넌': 127, '널': 128, '넘': 129, '넣': 130, '네': 131, '넷': 132, '녀': 133, '녁': 134, '년': 135, '념': 136, '녔': 137, '노': 138, '녹': 139, '논': 140, '놀': 141, '놋': 142, '농': 143, '높': 144, '놓': 145, '놔': 146, '놨': 147, '누': 148, '눅': 149, '눈': 150, '눠': 151, '뉴': 152, '느': 153, '는': 154, '늘': 155, '늙': 156, '능': 157, '늦': 158, '니': 159, '닌': 160, '닐': 161, '님': 162, '닙': 163, '다': 164, '닥': 165, '단': 166, '닫': 167, '달': 168, '닭': 169, '담': 170, '답': 171, '당': 172, '대': 173, '댓': 174, '더': 175, '덕': 176, '던': 177, '덟': 178, '덥': 179, '덩': 180, '데': 181, '도': 182, '독': 183, '돈': 184, '돌': 185, '동': 186, '돼': 187, '됐': 188, '되': 189, '된': 190, '될': 191, '됩': 192, '두': 193, '둘': 194, '둡': 195, '뒷': 196, '드': 197, '득': 198, '든': 199, '듣': 200, '들': 201, '듭': 202, '듯': 203, '등': 204, '디': 205, '딕': 206, '딘': 207, '딪': 208, '따': 209, '딱': 210, '딴': 211, '딸': 212, '땅': 213, '때': 214, '땐': 215, '땜': 216, '떠': 217, '떡': 218, '떤': 219, '떨': 220, '떴': 221, '떻': 222, '또': 223, '똑': 224, '똘': 225, '뜨': 226, '뜬': 227, '뜸': 228, '뜻': 229, '띄': 230, '띡': 231, '라': 232, '락': 233, '란': 234, '랄': 235, '람': 236, '랍': 237, '랐': 238, '랑': 239, '래': 240, '랜': 241, '랠': 242, '램': 243, '랬': 244, '략': 245, '량': 246, '러': 247, '런': 248, '럴': 249, '럼': 250, '럽': 251, '렇': 252, '레': 253, '렉': 254, '렛': 255, '려': 256, '력': 257, '련': 258, '렬': 259, '렸': 260, '령': 261, '례': 262, '로': 263, '록': 264, '론': 265, '롭': 266, '롯': 267, '료': 268, '루': 269, '룬': 270, '룰': 271, '룻': 272, '뤄': 273, '류': 274, '륙': 275, '륭': 276, '르': 277, '른': 278, '를': 279, '름': 280, '릅': 281, '리': 282, '린': 283, '릴': 284, '림': 285, '립': 286, '마': 287, '막': 288, '만': 289, '많': 290, '말': 291, '맘': 292, '맛': 293, '망': 294, '맞': 295, '맡': 296, '매': 297, '맥': 298, '맨': 299, '맹': 300, '맺': 301, '머': 302, '먹': 303, '먼': 304, '멀': 305, '메': 306, '멘': 307, '멜': 308, '멤': 309, '멧': 310, '며': 311, '면': 312, '멸': 313, '명': 314, '몇': 315, '모': 316, '목': 317, '몬': 318, '몰': 319, '몸': 320, '못': 321, '무': 322, '문': 323, '묻': 324, '물': 325, '뭉': 326, '뭐': 327, '뭔': 328, '뭘': 329, '뭡': 330, '뮤': 331, '므': 332, '믄': 333, '미': 334, '민': 335, '믿': 336, '밀': 337, '밌': 338, '밑': 339, '바': 340, '박': 341, '밖': 342, '반': 343, '받': 344, '발': 345, '밟': 346, '밤': 347, '밥': 348, '방': 349, '배': 350, '백': 351, '뱀': 352, '뱅': 353, '버': 354, '번': 355, '벌': 356, '법': 357, '벗': 358, '베': 359, '벨': 360, '벳': 361, '벼': 362, '벽': 363, '변': 364, '별': 365, '병': 366, '보': 367, '복': 368, '본': 369, '볼': 370, '봉': 371, '봐': 372, '봤': 373, '뵙': 374, '부': 375, '북': 376, '분': 377, '불': 378, '붓': 379, '붕': 380, '붙': 381, '뷰': 382, '브': 383, '비': 384, '빈': 385, '빌': 386, '빛': 387, '빠': 388, '빨': 389, '빵': 390, '빼': 391, '뻐': 392, '뻗': 393, '뼈': 394, '뽑': 395, '뿌': 396, '뿐': 397, '쁘': 398, '쁜': 399, '쁠': 400, '삐': 401, '사': 402, '산': 403, '살': 404, '삶': 405, '삼': 406, '샀': 407, '상': 408, '새': 409, '색': 410, '샘': 411, '생': 412, '샤': 413, '서': 414, '석': 415, '선': 416, '설': 417, '섬': 418, '섯': 419, '성': 420, '세': 421, '셈': 422, '셔': 423, '션': 424, '셨': 425, '소': 426, '속': 427, '손': 428, '솔': 429, '솜': 430, '송': 431, '쇠': 432, '수': 433, '숙': 434, '순': 435, '술': 436, '숨': 437, '숭': 438, '숱': 439, '쉬': 440, '쉽': 441, '슈': 442, '스': 443, '슨': 444, '슬': 445, '슴': 446, '습': 447, '슷': 448, '승': 449, '시': 450, '식': 451, '신': 452, '실': 453, '싫': 454, '심': 455, '십': 456, '싱': 457, '싶': 458, '싸': 459, '쌌': 460, '쌓': 461, '써': 462, '썼': 463, '쏘': 464, '쓰': 465, '쓴': 466, '쓸': 467, '씀': 468, '씁': 469, '씨': 470, '씩': 471, '씬': 472, '씰': 473, '아': 474, '악': 475, '안': 476, '앉': 477, '않': 478, '알': 479, '암': 480, '압': 481, '았': 482, '앙': 483, '앞': 484, '애': 485, '앤': 486, '앨': 487, '앵': 488, '야': 489, '약': 490, '양': 491, '얘': 492, '어': 493, '억': 494, '언': 495, '얻': 496, '얼': 497, '엄': 498, '업': 499, '없': 500, '엇': 501, '었': 502, '엎': 503, '에': 504, '엔': 505, '엘': 506, '여': 507, '역': 508, '연': 509, '열': 510, '염': 511, '엽': 512, '였': 513, '영': 514, '옆': 515, '예': 516, '옛': 517, '오': 518, '옥': 519, '온': 520, '올': 521, '옮': 522, '옵': 523, '옷': 524, '와': 525, '완': 526, '왔': 527, '왕': 528, '왜': 529, '외': 530, '요': 531, '욕': 532, '용': 533, '우': 534, '욱': 535, '운': 536, '울': 537, '움': 538, '웁': 539, '웃': 540, '웅': 541, '워': 542, '원': 543, '월': 544, '웠': 545, '웨': 546, '웬': 547, '웹': 548, '위': 549, '윌': 550, '유': 551, '육': 552, '율': 553, '융': 554, '으': 555, '윽': 556, '은': 557, '을': 558, '음': 559, '응': 560, '의': 561, '이': 562, '인': 563, '일': 564, '읽': 565, '잃': 566, '임': 567, '입': 568, '있': 569, '자': 570, '작': 571, '잔': 572, '잖': 573, '잘': 574, '잠': 575, '잡': 576, '잤': 577, '장': 578, '재': 579, '쟁': 580, '쟤': 581, '저': 582, '적': 583, '전': 584, '절': 585, '젊': 586, '점': 587, '접': 588, '정': 589, '젖': 590, '제': 591, '젤': 592, '젯': 593, '져': 594, '졌': 595, '조': 596, '족': 597, '존': 598, '졸': 599, '좀': 600, '종': 601, '좋': 602, '좌': 603, '죄': 604, '죕': 605, '죠': 606, '주': 607, '죽': 608, '준': 609, '줄': 610, '중': 611, '줘': 612, '줬': 613, '쥐': 614, '즘': 615, '증': 616, '지': 617, '직': 618, '진': 619, '질': 620, '짐': 621, '집': 622, '짓': 623, '징': 624, '짜': 625, '짝': 626, '짤': 627, '짧': 628, '짱': 629, '째': 630, '쩌': 631, '쪼': 632, '쪽': 633, '쫌': 634, '쫓': 635, '쫙': 636, '쭈': 637, '쭉': 638, '쯤': 639, '찍': 640, '찜': 641, '차': 642, '착': 643, '찬': 644, '찮': 645, '찰': 646, '참': 647, '찾': 648, '채': 649, '책': 650, '챙': 651, '처': 652, '천': 653, '첨': 654, '청': 655, '체': 656, '쳐': 657, '쳤': 658, '초': 659, '촌': 660, '촛': 661, '총': 662, '최': 663, '추': 664, '축': 665, '출': 666, '충': 667, '춰': 668, '취': 669, '층': 670, '치': 671, '칙': 672, '친': 673, '칠': 674, '침': 675, '칩': 676, '칭': 677, '카': 678, '칸': 679, '칼': 680, '캅': 681, '캐': 682, '캡': 683, '커': 684, '컨': 685, '컴': 686, '케': 687, '켜': 688, '켰': 689, '코': 690, '콕': 691, '콘': 692, '콜': 693, '콩': 694, '쿠': 695, '쿨': 696, '크': 697, '큰': 698, '큼': 699, '키': 700, '킨': 701, '킬': 702, '킴': 703, '타': 704, '탁': 705, '탄': 706, '탈': 707, '탐': 708, '탔': 709, '탕': 710, '태': 711, '터': 712, '턴': 713, '테': 714, '텐': 715, '토': 716, '톨': 717, '통': 718, '퇴': 719, '투': 720, '툰': 721, '툴': 722, '트': 723, '특': 724, '튼': 725, '틀': 726, '티': 727, '팅': 728, '파': 729, '판': 730, '팔': 731, '패': 732, '팰': 733, '팽': 734, '퍼': 735, '펀': 736, '페': 737, '펫': 738, '펴': 739, '편': 740, '평': 741, '폐': 742, '포': 743, '폰': 744, '표': 745, '푸': 746, '풀': 747, '품': 748, '풍': 749, '퓨': 750, '프': 751, '플': 752, '픔': 753, '피': 754, '픽': 755, '필': 756, '핍': 757, '하': 758, '학': 759, '한': 760, '할': 761, '함': 762, '합': 763, '항': 764, '해': 765, '핵': 766, '핸': 767, '했': 768, '행': 769, '향': 770, '허': 771, '헌': 772, '험': 773, '헤': 774, '헬': 775, '혀': 776, '혁': 777, '현': 778, '협': 779, '형': 780, '혜': 781, '호': 782, '혹': 783, '혼': 784, '홀': 785, '홈': 786, '홉': 787, '홍': 788, '화': 789, '확': 790, '환': 791, '활': 792, '황': 793, '회': 794, '획': 795, '효': 796, '후': 797, '훈': 798, '훌': 799, '훨': 800, '휘': 801, '흐': 802, '흘': 803, '희': 804, '흰': 805, '히': 806, '힘': 807}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 14, 1, 18, 12, 18], [7, 1, 18, 24, 7, 9, 16, 23, 18, 12, 1, 18, 25, 18, 1, 7, 10], [18, 7, 10, 18], [18, 1, 7, 8, 1, 18, 24, 7, 9, 16, 23, 18, 1, 19, 19, 1, 7, 1, 16, 1, 18, 9], [7, 12, 9, 8, 1, 19, 25, 18, 1, 25, 7, 18, 16, 14, 23, 1, 18, 7, 1, 11, 13, 18]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in word_list: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 562, 355, 3, 507, 280, 504, 2], [1, 68, 3, 474, 751, 9, 159, 443, 706, 555, 263, 3, 507, 769, 558, 3, 15, 164, 2], [1, 527, 22, 199, 531, 2], [1, 474, 3, 69, 80, 3, 474, 751, 9, 159, 443, 706, 504, 3, 618, 588, 3, 12, 3, 433, 3, 569, 154, 2], [1, 68, 247, 159, 80, 3, 618, 764, 562, 3, 760, 58, 504, 414, 375, 712, 3, 500, 74, 3, 214, 323, 504, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in target:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[562, 355, 3, 507, 280, 504, 2], [68, 3, 474, 751, 9, 159, 443, 706, 555, 263, 3, 507, 769, 558, 3, 15, 164, 2], [527, 22, 199, 531, 2], [474, 3, 69, 80, 3, 474, 751, 9, 159, 443, 706, 504, 3, 618, 588, 3, 12, 3, 433, 3, 569, 154, 2], [68, 247, 159, 80, 3, 618, 764, 562, 3, 760, 58, 504, 414, 375, 712, 3, 500, 74, 3, 214, 323, 504, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in target:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in word_list])\n",
    "max_tar_len = max([len(line) for line in target])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2726 samples, validate on 682 samples\n",
      "Epoch 1/200\n",
      "2726/2726 [==============================] - 20s 7ms/sample - loss: 2.8729 - val_loss: 1.2101\n",
      "Epoch 2/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.8484 - val_loss: 0.9709\n",
      "Epoch 3/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.7118 - val_loss: 0.8676\n",
      "Epoch 4/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.6601 - val_loss: 0.8220\n",
      "Epoch 5/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.6288 - val_loss: 0.7984\n",
      "Epoch 6/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.6093 - val_loss: 0.7796\n",
      "Epoch 7/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.5958 - val_loss: 0.7755\n",
      "Epoch 8/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.5846 - val_loss: 0.7650\n",
      "Epoch 9/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.5738 - val_loss: 0.7510\n",
      "Epoch 10/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.5628 - val_loss: 0.7375\n",
      "Epoch 11/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5515 - val_loss: 0.7317\n",
      "Epoch 12/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5406 - val_loss: 0.7213\n",
      "Epoch 13/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5305 - val_loss: 0.7180\n",
      "Epoch 14/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5213 - val_loss: 0.7080\n",
      "Epoch 15/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5129 - val_loss: 0.7016\n",
      "Epoch 16/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.5049 - val_loss: 0.7019\n",
      "Epoch 17/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4978 - val_loss: 0.6953\n",
      "Epoch 18/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4908 - val_loss: 0.6892\n",
      "Epoch 19/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4840 - val_loss: 0.6894\n",
      "Epoch 20/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4777 - val_loss: 0.6828\n",
      "Epoch 21/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4718 - val_loss: 0.6816\n",
      "Epoch 22/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4676 - val_loss: 0.6743\n",
      "Epoch 23/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4609 - val_loss: 0.6751\n",
      "Epoch 24/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4552 - val_loss: 0.6791\n",
      "Epoch 25/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4502 - val_loss: 0.6745\n",
      "Epoch 26/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4480 - val_loss: 0.6645\n",
      "Epoch 27/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4407 - val_loss: 0.6774\n",
      "Epoch 28/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4364 - val_loss: 0.6601\n",
      "Epoch 29/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4350 - val_loss: 0.6677\n",
      "Epoch 30/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4278 - val_loss: 0.6597\n",
      "Epoch 31/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4239 - val_loss: 0.6675\n",
      "Epoch 32/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4213 - val_loss: 0.6590\n",
      "Epoch 33/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4159 - val_loss: 0.6545\n",
      "Epoch 34/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.4123 - val_loss: 0.6704\n",
      "Epoch 35/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4088 - val_loss: 0.6500\n",
      "Epoch 36/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4051 - val_loss: 0.6517\n",
      "Epoch 37/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.4014 - val_loss: 0.6504\n",
      "Epoch 38/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3978 - val_loss: 0.6508\n",
      "Epoch 39/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3948 - val_loss: 0.6667\n",
      "Epoch 40/200\n",
      "2726/2726 [==============================] - 17s 6ms/sample - loss: 0.3913 - val_loss: 0.6458\n",
      "Epoch 41/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3883 - val_loss: 0.6505\n",
      "Epoch 42/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3854 - val_loss: 0.6448\n",
      "Epoch 43/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3820 - val_loss: 0.6482\n",
      "Epoch 44/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3792 - val_loss: 0.6457\n",
      "Epoch 45/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3764 - val_loss: 0.6484\n",
      "Epoch 46/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3736 - val_loss: 0.6420\n",
      "Epoch 47/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3706 - val_loss: 0.6428\n",
      "Epoch 48/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3679 - val_loss: 0.6402\n",
      "Epoch 49/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3652 - val_loss: 0.6488\n",
      "Epoch 50/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3628 - val_loss: 0.6459\n",
      "Epoch 51/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3602 - val_loss: 0.6399\n",
      "Epoch 52/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3573 - val_loss: 0.6420\n",
      "Epoch 53/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3548 - val_loss: 0.6454\n",
      "Epoch 54/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3525 - val_loss: 0.6395\n",
      "Epoch 55/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3508 - val_loss: 0.6401\n",
      "Epoch 56/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3487 - val_loss: 0.6465\n",
      "Epoch 57/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3456 - val_loss: 0.6477\n",
      "Epoch 58/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3442 - val_loss: 0.6403\n",
      "Epoch 59/200\n",
      "2726/2726 [==============================] - 16s 6ms/sample - loss: 0.3411 - val_loss: 0.6451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x5b4157c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience = 5) # 조기종료 콜백함수 정의\n",
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=200, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(64,))\n",
    "decoder_state_input_c = Input(shape=(64,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: ㄸ ㅇㅍㄱㄴㅅㅌ ㄴㅇㅅㄴ\n",
      "정답 문장: 또 아프가니스탄 내에서는\n",
      "번역기가 번역한 문장: 이 사람들이 있었습니다\n",
      "-----------------------------------\n",
      "입력 문장: ㄱㄲ ㄱㄱㄷ ㄱ ㅍㅇㅇㄱ\n",
      "정답 문장: 긍까 기간도 긴 편이었고\n",
      "번역기가 번역한 문장: 이 사람들이 있었습니다\n",
      "-----------------------------------\n",
      "입력 문장: ㅇㄷㄹ ㅅㄱ\n",
      "정답 문장: 엎드릴 수가\n",
      "번역기가 번역한 문장: 이 사람들이\n",
      "-----------------------------------\n",
      "입력 문장: ㅇㄹㅇㅌㅇㅅ ㄱㅎㅅ\n",
      "정답 문장: 오리엔테이션 겸해서\n",
      "번역기가 번역한 문장: 이 사람들이 있었습니다\n",
      "-----------------------------------\n",
      "입력 문장: ㅍㅋㅅㅌㅇ ㄱㅊㅅ ㄱㄷ\n",
      "정답 문장: 파키스탄을 거쳐서 갔다\n",
      "번역기가 번역한 문장: 이 사람들이 있었습니다\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "i = 0\n",
    "for seq_index in [105,42,1502,135,5]: # 입력 문장의 인덱스\n",
    "    i+=1\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', word_list[seq_index])\n",
    "    print('정답 문장:', target[seq_index][1:len(target[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
