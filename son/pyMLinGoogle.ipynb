{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jzfeKDMP2vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kss\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elCn_RKOQYBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "   \n",
        "# PyDrive Authentication\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bibi9-QvUk97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_id = '1wV4k91wu5NVNuC-KHSBv2xxqpFSVd0gt'\n",
        "   \n",
        "def ListFolder(parent):\n",
        "    filelist=[]\n",
        "    file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % parent}).GetList()\n",
        "    for f in file_list:\n",
        "        if f['mimeType']=='application/vnd.google-apps.folder': # if folder\n",
        "            filelist.append({\"id\":f['id'],\"title\":f['title'],\"list\":ListFolder(f['id'])})\n",
        "        else:\n",
        "            filelist.append({\"title\":f['title'],\"id\":f['id']})\n",
        "    return filelist\n",
        "   \n",
        "   \n",
        "file_lists = ListFolder(folder_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2pEcF25Ve13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "af288c25-9495-44a0-c152-2756792a7878"
      },
      "source": [
        "file_lists_from_drive = ListFolder(folder_id)\n",
        "   \n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/my_sample_data')\n",
        "try:\n",
        "    os.makedirs(local_download_path)\n",
        "except: pass\n",
        "   \n",
        "for file in file_lists_from_drive:\n",
        "    print('title: %s, id: %s' % (file['title'], file['id']))\n",
        "    fname = os.path.join(local_download_path, file['title'])\n",
        "    print('downloading to {}'.format(fname))\n",
        "    f_ = drive.CreateFile({'id': file['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "    print(fname)\n",
        "    file_lists.append(fname)\n",
        "   \n",
        "# print file lists\n",
        "print(file_lists)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: CM.txt, id: 1QHq6p67jpE56AI5c1HPhJifTpSYaB9Gl\n",
            "downloading to /root/my_sample_data/CM.txt\n",
            "/root/my_sample_data/CM.txt\n",
            "[{'title': 'CM.txt', 'id': '1QHq6p67jpE56AI5c1HPhJifTpSYaB9Gl'}, '/root/my_sample_data/CM.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7eoBsyzRXnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('../root/my_sample_data/CM.txt', encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad96K8WQR4YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import kss\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "choseong_list = [char for char in \"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G5it_d8R4au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "abc01064-e261-4555-dd61-9b4c4e1b61ff"
      },
      "source": [
        "i=0\n",
        "gogo= 0\n",
        "sentence_list = []\n",
        "lowCount = 7\n",
        "longCount = 20\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line: break\n",
        "    if \"<!DOCTYPE\" in line:\n",
        "        gogo = 0\n",
        "    if \"<text>\" in line:\n",
        "        gogo = 1\n",
        "    if \"</text>\" in line:\n",
        "        gogo = 0\n",
        "    if gogo == 0: continue\n",
        "    if line != '\\n':\n",
        "        i=i+1\n",
        "        line = line.replace(u'\\u3000', u\"\")\n",
        "        line = line.replace(u'\\x80', u\"\")\n",
        "        line = line.replace(u'\\x84', u\"\")\n",
        "        line = line.replace(u'\\x88', u\"\")\n",
        "        line = line.replace(u'\\x8a', u\"\")\n",
        "        line = line.replace(u'\\x8b', u\"\")\n",
        "        line = line.replace(u'\\x8c', u\"\")\n",
        "        line = line.replace(u'\\x90', u\"\")\n",
        "        line = line.replace(u'\\x94', u\"\")\n",
        "        line = line.replace(u'\\x97', u\"\")\n",
        "        line = line.replace(u'\\x9a', u\"\")\n",
        "        line = line.replace(u'\\xa0', u\"\")\n",
        "        line = line.replace(u'\\x9e', u\"\")\n",
        "        line = line.replace(u'\\x9d', u\"\")\n",
        "        line = re.sub('<trunc>.+?<\\/trunc>', '', line, 0, re.S)\n",
        "        line = re.sub('<.+?>', '', line, 0, re.S)\n",
        "        line = re.sub('\\(.+?\\)', '', line, 0, re.S)\n",
        "        line = re.sub('\\[.+?\\]', '', line, 0, re.S)\n",
        "        line = re.sub('\\{.+?\\}', '', line, 0, re.S)\n",
        "        line = re.sub('《.+?》', '', line, 0, re.S)\n",
        "        line = re.sub('[-§¨©°³¸¼´·²～¯¥=+,#/ꮀ\\?:;¡^$@*\\\"※~&%ㆍ!『』\\‘|\\(\\)\\[\\]\\<\\>`\\'…《》\\{\\}_「」±√]', '', line,0,re.S)\n",
        "        line = line.lower()\n",
        "        line = re.sub('[a-z]', '', line, 0, re.S)\n",
        "        line = re.sub('[A-Z]', '', line, 0, re.S)\n",
        "        line_split = kss.split_sentences(line)\n",
        "        temp_line = []\n",
        "        for l_sp in line_split:\n",
        "            if len(l_sp) >lowCount and len(l_sp) <longCount:\n",
        "                temp_line.append(l_sp)\n",
        "        sentence_list+=temp_line\n",
        "f.close()\n",
        "print(len(sentence_list))\n",
        "print(sentence_list[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91444\n",
            "['뭘 좀 올려야지.', '뭘 좀 올렸어.', '다시 돌려 앞으로.', '우리가족은 아빠 엄마 오빠 나.', '목소리 왜 이케 작어.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TmUScDGR4dU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fe309996-2b1e-4a4e-f6f3-53cba2467fca"
      },
      "source": [
        "#print(sentence_list[:5])\n",
        "word_list = []\n",
        "target = []\n",
        "for sentence in sentence_list:\n",
        "    sen2word = \"\"\n",
        "    for word in sentence:\n",
        "        if re.match('[가-힣]',word) is not None :\n",
        "            sen2word+=choseong_list[(int)((ord(word)-ord('가'))/(21*28))]\n",
        "        else :\n",
        "            sen2word+=word\n",
        "    target.append(sentence+'\\n')\n",
        "    word_list.append(sen2word+'\\n')\n",
        "    \n",
        "#print(target[:5])\n",
        "#print(word_list[:5])\n",
        "src_vocab=set()\n",
        "for words in word_list: # 1줄씩 읽음\n",
        "    for char in words: # 1개의 글자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab=set()\n",
        "for line in target:\n",
        "    for char in line:\n",
        "        tar_vocab.add(char)\n",
        "        \n",
        "src_vocab_size = len(src_vocab)+1\n",
        "tar_vocab_size = len(tar_vocab)+1\n",
        "#print(src_vocab_size)\n",
        "#print(tar_vocab_size)\n",
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab)\n",
        "print(tar_vocab[10:20])\n",
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "#print(src_to_index)\n",
        "#print(tar_to_index)\n",
        "encoder_input = []\n",
        "for line in word_list: #입력 데이터에서 1줄씩 문장을 읽음\n",
        "    temp_X = []\n",
        "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
        "    encoder_input.append(temp_X[:-1])\n",
        "print(encoder_input[:5])\n",
        "decoder_input = []\n",
        "for line in word_list:\n",
        "    temp_X = []\n",
        "    for w in line:\n",
        "      temp_X.append(src_to_index[w])\n",
        "    decoder_input.append(temp_X)\n",
        "print(decoder_input[:5])\n",
        "decoder_target = []\n",
        "for line in target:\n",
        "    temp_X = []\n",
        "    for w in line:\n",
        "        temp_X.append(tar_to_index[w])\n",
        "    decoder_target.append(temp_X)\n",
        "print(decoder_target[:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
            "['7', '8', '9', '가', '각', '간', '갇', '갈', '감', '갑']\n",
            "[[20, 2, 26, 2, 25, 19, 25, 26, 3], [20, 2, 26, 2, 25, 19, 25, 3], [17, 23, 2, 17, 19, 2, 25, 25, 19, 3], [25, 19, 14, 26, 25, 2, 25, 22, 2, 25, 20, 2, 25, 22, 2, 16, 3], [20, 23, 19, 2, 25, 2, 25, 29, 2, 26, 25, 3]]\n",
            "[[20, 2, 26, 2, 25, 19, 25, 26, 3, 1], [20, 2, 26, 2, 25, 19, 25, 3, 1], [17, 23, 2, 17, 19, 2, 25, 25, 19, 3, 1], [25, 19, 14, 26, 25, 2, 25, 22, 2, 25, 20, 2, 25, 22, 2, 16, 3, 1], [20, 23, 19, 2, 25, 2, 25, 29, 2, 26, 25, 3, 1]]\n",
            "[[636, 2, 1168, 2, 1041, 506, 985, 1201, 3, 1], [636, 2, 1168, 2, 1041, 513, 998, 3, 1], [308, 890, 2, 356, 506, 2, 977, 1095, 517, 3, 1], [1065, 557, 14, 1165, 1097, 2, 964, 743, 2, 1005, 565, 2, 1038, 743, 2, 211, 3, 1], [616, 845, 557, 2, 1055, 2, 1109, 1364, 2, 1125, 998, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVOSDWkBTDRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "51b8e539-f160-4078-be83-7408d317ac09"
      },
      "source": [
        "max_src_len = max([len(line) for line in word_list])-1\n",
        "max_tar_len = max([len(line) for line in target])\n",
        "print(max_src_len)\n",
        "print(max_tar_len)\n",
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
        "\n",
        "encoder_inputer = encoder_input\n",
        "decoder_inputer = decoder_input\n",
        "decoder_targeter = decoder_target\n",
        "\n",
        "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units=256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
        "encoder_states = [state_h, state_c]\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태.\n",
        "\n",
        "decoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrl8V_-vTBW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = to_categorical(encoder_inputer, num_classes=src_vocab_size)\n",
        "decoder_input = to_categorical(decoder_inputer, num_classes=src_vocab_size)\n",
        "decoder_target = to_categorical(decoder_targeter, num_classes=tar_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztp2A9oOYL00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85245c1b-857b-42a3-803a-fc49f1e1b5cb"
      },
      "source": [
        "model.fit(x=[encoder_input, decoder_input], y=decoder_target,  epochs=50, validation_split=0.2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 73155 samples, validate on 18289 samples\n",
            "Epoch 1/50\n",
            "73155/73155 [==============================] - 44s 600us/sample - loss: 1.5370 - val_loss: 1.1822\n",
            "Epoch 2/50\n",
            "73155/73155 [==============================] - 26s 355us/sample - loss: 1.1407 - val_loss: 1.1327\n",
            "Epoch 3/50\n",
            "73155/73155 [==============================] - 26s 354us/sample - loss: 1.0897 - val_loss: 1.1372\n",
            "Epoch 4/50\n",
            "73155/73155 [==============================] - 26s 354us/sample - loss: 1.0468 - val_loss: 1.0709\n",
            "Epoch 5/50\n",
            "73155/73155 [==============================] - 28s 380us/sample - loss: 1.0128 - val_loss: 1.0437\n",
            "Epoch 6/50\n",
            "73155/73155 [==============================] - 35s 484us/sample - loss: 0.9800 - val_loss: 1.0519\n",
            "Epoch 7/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.9529 - val_loss: 1.0202\n",
            "Epoch 8/50\n",
            "73155/73155 [==============================] - 35s 479us/sample - loss: 0.9309 - val_loss: 1.0036\n",
            "Epoch 9/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.9113 - val_loss: 0.9910\n",
            "Epoch 10/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.8931 - val_loss: 0.9887\n",
            "Epoch 11/50\n",
            "73155/73155 [==============================] - 35s 476us/sample - loss: 0.8775 - val_loss: 0.9792\n",
            "Epoch 12/50\n",
            "73155/73155 [==============================] - 35s 479us/sample - loss: 0.8632 - val_loss: 0.9818\n",
            "Epoch 13/50\n",
            "73155/73155 [==============================] - 35s 478us/sample - loss: 0.8491 - val_loss: 0.9718\n",
            "Epoch 14/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.8355 - val_loss: 0.9727\n",
            "Epoch 15/50\n",
            "73155/73155 [==============================] - 35s 483us/sample - loss: 0.8237 - val_loss: 0.9705\n",
            "Epoch 16/50\n",
            "73155/73155 [==============================] - 35s 478us/sample - loss: 0.8133 - val_loss: 0.9701\n",
            "Epoch 17/50\n",
            "73155/73155 [==============================] - 35s 477us/sample - loss: 0.8018 - val_loss: 0.9717\n",
            "Epoch 18/50\n",
            "73155/73155 [==============================] - 35s 477us/sample - loss: 0.7916 - val_loss: 0.9689\n",
            "Epoch 19/50\n",
            "73155/73155 [==============================] - 35s 476us/sample - loss: 0.7820 - val_loss: 0.9747\n",
            "Epoch 20/50\n",
            "73155/73155 [==============================] - 35s 476us/sample - loss: 0.7721 - val_loss: 0.9766\n",
            "Epoch 21/50\n",
            "73155/73155 [==============================] - 35s 475us/sample - loss: 0.7632 - val_loss: 0.9817\n",
            "Epoch 22/50\n",
            "73155/73155 [==============================] - 35s 476us/sample - loss: 0.7539 - val_loss: 0.9758\n",
            "Epoch 23/50\n",
            "73155/73155 [==============================] - 35s 476us/sample - loss: 0.7450 - val_loss: 0.9785\n",
            "Epoch 24/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.7366 - val_loss: 0.9816\n",
            "Epoch 25/50\n",
            "73155/73155 [==============================] - 35s 478us/sample - loss: 0.7275 - val_loss: 0.9830\n",
            "Epoch 26/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.7189 - val_loss: 0.9947\n",
            "Epoch 27/50\n",
            "73155/73155 [==============================] - 35s 479us/sample - loss: 0.7111 - val_loss: 0.9987\n",
            "Epoch 28/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.7035 - val_loss: 0.9995\n",
            "Epoch 29/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.6961 - val_loss: 0.9983\n",
            "Epoch 30/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.6883 - val_loss: 1.0107\n",
            "Epoch 31/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.6809 - val_loss: 1.0136\n",
            "Epoch 32/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.6742 - val_loss: 1.0203\n",
            "Epoch 33/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.6673 - val_loss: 1.0178\n",
            "Epoch 34/50\n",
            "73155/73155 [==============================] - 35s 479us/sample - loss: 0.6608 - val_loss: 1.0296\n",
            "Epoch 35/50\n",
            "73155/73155 [==============================] - 35s 482us/sample - loss: 0.6548 - val_loss: 1.0346\n",
            "Epoch 36/50\n",
            "73155/73155 [==============================] - 35s 483us/sample - loss: 0.6484 - val_loss: 1.0450\n",
            "Epoch 37/50\n",
            "73155/73155 [==============================] - 35s 484us/sample - loss: 0.6430 - val_loss: 1.0420\n",
            "Epoch 38/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.6366 - val_loss: 1.0571\n",
            "Epoch 39/50\n",
            "73155/73155 [==============================] - 35s 484us/sample - loss: 0.6312 - val_loss: 1.0554\n",
            "Epoch 40/50\n",
            "73155/73155 [==============================] - 35s 484us/sample - loss: 0.6251 - val_loss: 1.0584\n",
            "Epoch 41/50\n",
            "73155/73155 [==============================] - 35s 485us/sample - loss: 0.6197 - val_loss: 1.0732\n",
            "Epoch 42/50\n",
            "73155/73155 [==============================] - 35s 483us/sample - loss: 0.6152 - val_loss: 1.0782\n",
            "Epoch 43/50\n",
            "73155/73155 [==============================] - 35s 481us/sample - loss: 0.6097 - val_loss: 1.0816\n",
            "Epoch 44/50\n",
            "73155/73155 [==============================] - 35s 483us/sample - loss: 0.6049 - val_loss: 1.0923\n",
            "Epoch 45/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.6002 - val_loss: 1.1001\n",
            "Epoch 46/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.5962 - val_loss: 1.0953\n",
            "Epoch 47/50\n",
            "73155/73155 [==============================] - 35s 480us/sample - loss: 0.5917 - val_loss: 1.1056\n",
            "Epoch 48/50\n",
            "73155/73155 [==============================] - 35s 484us/sample - loss: 0.5877 - val_loss: 1.1066\n",
            "Epoch 49/50\n",
            "73155/73155 [==============================] - 35s 483us/sample - loss: 0.5832 - val_loss: 1.1212\n",
            "Epoch 50/50\n",
            "73155/73155 [==============================] - 36s 489us/sample - loss: 0.5787 - val_loss: 1.1223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f02600a34e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU6LN4FAbmrA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "1f9df521-0846-45d7-e12f-7050b77e9593"
      },
      "source": [
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "notebooks_dir_name = 'MLTEXT'\n",
        "drive.mount('/content/gdrive')\n",
        "notebooks_base_dir = path.join('./gdrive/My Drive/', notebooks_dir_name)\n",
        "if not path.exists(notebooks_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "model.save(path.join(notebooks_base_dir, \"seq2seq.krs\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: ./gdrive/My Drive/MLTEXT/seq2seq.krs/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4dce8957489c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebooks_base_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seq2seq.krs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mencoder_decoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_decoder_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhWXTgXdiGh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYPSGxI-jJKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "decoder_states = [state_h, state_c]\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOzpC1dHjKQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nox-fAzTjLqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str2cho(input_str):\n",
        "    #문장을 입력받으면 초성 문자열로반환하는 코드\n",
        "    cho = \"\"\n",
        "    for word in input_str:\n",
        "        if re.match('[가-힣]',word) is not None :\n",
        "            cho+=(choseong_list[(int)((ord(word)-ord('가'))/(21*28))])\n",
        "        else :\n",
        "            cho+=(word)\n",
        "    return cho"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W2wUDQwjMg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq,strWORD):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # 첫글자에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1, src_vocab_size))\n",
        "    target_seq[0, 0, src_to_index[str2cho(strWORD[0])]] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    cho_sen = \"\"\n",
        "    cho_sen+=str2cho(strWORD[0])\n",
        "    itr = 0\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 입력 글자를 넘으면 중단.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > len(strWORD)):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "          itr+=1\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
        "        target_seq = np.zeros((1, 1, src_vocab_size))\n",
        "        \n",
        "        target_seq[0, 0, src_to_index[str2cho((strWORD[itr]))]] = 1.\n",
        "        cho_sen+=str2cho(strWORD[itr])\n",
        "        # 상태를 업데이트 합니다.\n",
        "        states_value = [h, c]\n",
        "    #print(cho_sen)\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA3KkbtkjNrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "b7e1d1e4-91c9-4c8b-f20b-58abfa8c30b1"
      },
      "source": [
        "i = 0\n",
        "for seq_index in [105,70000,15002,40015,5]: # 입력 문장의 인덱스\n",
        "    i+=1\n",
        "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq,word_list[seq_index])\n",
        "    print(35 * \"-\")\n",
        "    print('입력 문장:', word_list[seq_index])\n",
        "    print('정답 문장:', target[seq_index][:len(target[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)]) # '\\n'을 빼고 출력"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "입력 문장: ㄲ ㄱㅊㅇ ㅈㅅㅇㄴㄷ\n",
            "\n",
            "정답 문장: 꽤 괜찮은 점수였는데\n",
            "번역기가 번역한 문장: 꼭 기찮의 자수이니데\n",
            "\n",
            "-----------------------------------\n",
            "입력 문장: ㅈㅇㅁㅌㄹㅇㅅ ㅂㅇ ㄱㄱ ㄸㄱㅇㅇ.\n",
            "\n",
            "정답 문장: 지오메트리에서 배운 것과 똑같아요.\n",
            "번역기가 번역한 문장: 지오메트리에서 방운 것과 똑같애요.\n",
            "\n",
            "-----------------------------------\n",
            "입력 문장: ㄱㄹ ㄱ ㅊ ㅂㅉ ㄱㄹㄱ\n",
            "\n",
            "정답 문장: 그런 게 첫 번째 그렇고\n",
            "번역기가 번역한 문장: 그런 거 첫 번째 그렇고\n",
            "\n",
            "-----------------------------------\n",
            "입력 문장: ㅎㅌㅅㅇ ㅅㄴ ㅇㄴㄷ\n",
            "\n",
            "정답 문장: 형태소일 수는 있는데\n",
            "번역기가 번역한 문장: 화태소에 사는 있는데\n",
            "\n",
            "-----------------------------------\n",
            "입력 문장: ㅇㄹㄱ ㄴ ㅁㅇㄷ\n",
            "\n",
            "정답 문장: 이렇게 네 명인데\n",
            "번역기가 번역한 문장: 이렇게 난 먹인다\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aIrD8vOo0Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "26cb685d-58d3-4495-b4e3-8ce66bc0bd68"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None, 33)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None, 33)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 256), (None, 296960      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  296960      input_4[0][0]                    \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 1637)   420709      lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,014,629\n",
            "Trainable params: 1,014,629\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}